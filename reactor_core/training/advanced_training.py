"""
Advanced Training Methods for AGI OS - Reactor Core (Nervous System)
=====================================================================

Implements cutting-edge training paradigms:
- DPO (Direct Preference Optimization) - Preference learning without reward models
- RLHF (Reinforcement Learning from Human Feedback) - Full PPO pipeline
- FSDP (Fully Sharded Data Parallel) - Distributed training at scale
- Constitutional AI - Self-supervised safety alignment
- Curriculum Learning - Progressive difficulty scheduling
- Self-Supervised Learning - Learning from JARVIS interaction logs

ARCHITECTURE:
    Experience Buffer → Data Selector → Training Router
                                              │
                    ┌─────────────────────────┼─────────────────────────┐
                    │                         │                         │
                    ▼                         ▼                         ▼
              DPO Trainer             RLHF Pipeline            Constitutional AI
                    │                         │                         │
                    └─────────────────────────┼─────────────────────────┘
                                              ▼
                                    Unified Model Output
"""

from __future__ import annotations

import asyncio
import gc
import hashlib
import json
import logging
import math
import os
import random
import time
import uuid
from abc import ABC, abstractmethod
from collections import defaultdict, deque
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum, auto
from pathlib import Path
from typing import (
    Any,
    AsyncIterator,
    Awaitable,
    Callable,
    Deque,
    Dict,
    Generic,
    Iterator,
    List,
    Mapping,
    Optional,
    Protocol,
    Sequence,
    Set,
    Tuple,
    TypeVar,
    Union,
    cast,
)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset, IterableDataset

logger = logging.getLogger(__name__)

T = TypeVar("T")
ModelT = TypeVar("ModelT", bound=nn.Module)


# =============================================================================
# ENUMS AND CONSTANTS
# =============================================================================

class TrainingMethod(Enum):
    """Available training methods."""
    SFT = "sft"  # Supervised Fine-Tuning
    DPO = "dpo"  # Direct Preference Optimization
    RLHF = "rlhf"  # Reinforcement Learning from Human Feedback
    PPO = "ppo"  # Proximal Policy Optimization
    KTO = "kto"  # Kahneman-Tversky Optimization
    ORPO = "orpo"  # Odds Ratio Preference Optimization
    IPO = "ipo"  # Identity Preference Optimization
    CONSTITUTIONAL = "constitutional"  # Constitutional AI
    CURRICULUM = "curriculum"  # Curriculum Learning
    SELF_PLAY = "self_play"  # Self-Play Fine-Tuning


class SafetyTier(Enum):
    """Safety classification tiers for Constitutional AI."""
    SAFE = "safe"
    LOW_RISK = "low_risk"
    MEDIUM_RISK = "medium_risk"
    HIGH_RISK = "high_risk"
    CRITICAL = "critical"
    BLOCKED = "blocked"


class CurriculumStrategy(Enum):
    """Curriculum learning strategies."""
    LINEAR = "linear"  # Linear difficulty increase
    EXPONENTIAL = "exponential"  # Exponential difficulty
    ADAPTIVE = "adaptive"  # Adapt based on model performance
    SELF_PACED = "self_paced"  # Model controls its pace
    COMPETENCE_BASED = "competence_based"  # Based on competence metrics
    ANTI_CURRICULUM = "anti_curriculum"  # Hard examples first


class ExperiencePriority(Enum):
    """Priority levels for experience buffer."""
    CRITICAL = 1  # User corrections, safety violations
    HIGH = 2  # Explicit feedback, failures
    MEDIUM = 3  # Normal interactions
    LOW = 4  # Routine successful interactions
    BACKGROUND = 5  # Synthetic/augmented data


# =============================================================================
# DATA STRUCTURES
# =============================================================================

@dataclass
class PreferencePair:
    """A preference pair for DPO/RLHF training."""
    prompt: str
    chosen: str
    rejected: str
    chosen_score: float = 1.0
    rejected_score: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    source: str = "unknown"
    timestamp: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> Dict[str, Any]:
        return {
            "prompt": self.prompt,
            "chosen": self.chosen,
            "rejected": self.rejected,
            "chosen_score": self.chosen_score,
            "rejected_score": self.rejected_score,
            "metadata": self.metadata,
            "source": self.source,
            "timestamp": self.timestamp.isoformat(),
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "PreferencePair":
        data = data.copy()
        if "timestamp" in data and isinstance(data["timestamp"], str):
            data["timestamp"] = datetime.fromisoformat(data["timestamp"])
        return cls(**data)

    def compute_hash(self) -> str:
        """Compute deduplication hash."""
        content = f"{self.prompt}:{self.chosen}:{self.rejected}"
        return hashlib.sha256(content.encode()).hexdigest()[:16]


@dataclass
class Experience:
    """A single learning experience from JARVIS interactions."""
    id: str = field(default_factory=lambda: str(uuid.uuid4())[:12])
    prompt: str = ""
    response: str = ""
    reward: float = 0.0
    priority: ExperiencePriority = ExperiencePriority.MEDIUM
    correction: Optional[str] = None  # User correction if any
    feedback_type: str = "implicit"  # implicit, explicit, correction
    safety_tier: SafetyTier = SafetyTier.SAFE
    difficulty_score: float = 0.5  # For curriculum learning
    competence_required: float = 0.5
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

    def to_preference_pair(self) -> Optional[PreferencePair]:
        """Convert to preference pair if correction exists."""
        if self.correction:
            return PreferencePair(
                prompt=self.prompt,
                chosen=self.correction,
                rejected=self.response,
                chosen_score=1.0,
                rejected_score=self.reward,
                source="user_correction",
                metadata=self.metadata,
            )
        return None


@dataclass
class TrainingBatch:
    """A batch of training data with metadata."""
    input_ids: torch.Tensor
    attention_mask: torch.Tensor
    labels: Optional[torch.Tensor] = None
    chosen_input_ids: Optional[torch.Tensor] = None
    chosen_attention_mask: Optional[torch.Tensor] = None
    rejected_input_ids: Optional[torch.Tensor] = None
    rejected_attention_mask: Optional[torch.Tensor] = None
    rewards: Optional[torch.Tensor] = None
    advantages: Optional[torch.Tensor] = None
    old_log_probs: Optional[torch.Tensor] = None

    def to(self, device: torch.device) -> "TrainingBatch":
        """Move batch to device."""
        def move(t: Optional[torch.Tensor]) -> Optional[torch.Tensor]:
            return t.to(device) if t is not None else None

        return TrainingBatch(
            input_ids=move(self.input_ids),
            attention_mask=move(self.attention_mask),
            labels=move(self.labels),
            chosen_input_ids=move(self.chosen_input_ids),
            chosen_attention_mask=move(self.chosen_attention_mask),
            rejected_input_ids=move(self.rejected_input_ids),
            rejected_attention_mask=move(self.rejected_attention_mask),
            rewards=move(self.rewards),
            advantages=move(self.advantages),
            old_log_probs=move(self.old_log_probs),
        )


@dataclass
class TrainingMetrics:
    """Comprehensive training metrics."""
    step: int = 0
    epoch: int = 0
    loss: float = 0.0
    policy_loss: float = 0.0
    value_loss: float = 0.0
    reward_mean: float = 0.0
    reward_std: float = 0.0
    kl_divergence: float = 0.0
    entropy: float = 0.0
    learning_rate: float = 0.0
    gradient_norm: float = 0.0
    chosen_rewards: float = 0.0
    rejected_rewards: float = 0.0
    accuracy: float = 0.0
    safety_violations: int = 0
    curriculum_level: float = 0.0
    experience_buffer_size: int = 0
    samples_per_second: float = 0.0
    gpu_memory_used_gb: float = 0.0

    def to_dict(self) -> Dict[str, Any]:
        return {k: v for k, v in self.__dict__.items() if not k.startswith("_")}


# =============================================================================
# EXPERIENCE BUFFER - Prioritized Replay Memory
# =============================================================================

class ExperienceBuffer:
    """
    Prioritized experience replay buffer for continuous learning.

    Features:
    - Priority-based sampling (corrections > feedback > normal)
    - Temporal decay for old experiences
    - Deduplication with content hashing
    - Automatic quality filtering
    - Memory-efficient storage with compression
    - Async-safe operations
    """

    def __init__(
        self,
        max_size: int = 100000,
        priority_alpha: float = 0.6,  # Priority exponent
        temporal_decay: float = 0.999,  # Daily decay factor
        min_quality_score: float = 0.3,
        dedup_enabled: bool = True,
    ):
        self.max_size = max_size
        self.priority_alpha = priority_alpha
        self.temporal_decay = temporal_decay
        self.min_quality_score = min_quality_score
        self.dedup_enabled = dedup_enabled

        # Storage
        self._buffer: Deque[Experience] = deque(maxlen=max_size)
        self._priorities: Dict[str, float] = {}
        self._seen_hashes: Set[str] = set()
        self._lock = asyncio.Lock()

        # Statistics
        self._total_added = 0
        self._total_sampled = 0
        self._duplicates_rejected = 0
        self._quality_rejected = 0

    async def add(
        self,
        experience: Experience,
        priority_boost: float = 1.0,
    ) -> bool:
        """Add experience to buffer with priority."""
        async with self._lock:
            # Deduplication
            if self.dedup_enabled:
                exp_hash = hashlib.sha256(
                    f"{experience.prompt}:{experience.response}".encode()
                ).hexdigest()[:16]

                if exp_hash in self._seen_hashes:
                    self._duplicates_rejected += 1
                    return False
                self._seen_hashes.add(exp_hash)

            # Quality filter
            quality_score = self._compute_quality_score(experience)
            if quality_score < self.min_quality_score:
                self._quality_rejected += 1
                return False

            # Compute priority
            base_priority = self._get_base_priority(experience)
            priority = (base_priority * priority_boost * quality_score) ** self.priority_alpha

            # Add to buffer
            self._buffer.append(experience)
            self._priorities[experience.id] = priority
            self._total_added += 1

            # Cleanup old priorities if buffer is full
            if len(self._priorities) > self.max_size * 1.5:
                self._cleanup_priorities()

            return True

    async def add_batch(self, experiences: List[Experience]) -> int:
        """Add multiple experiences, returns count added."""
        added = 0
        for exp in experiences:
            if await self.add(exp):
                added += 1
        return added

    async def sample(
        self,
        batch_size: int,
        strategy: str = "priority",  # priority, uniform, recent
    ) -> List[Experience]:
        """Sample experiences from buffer."""
        async with self._lock:
            if len(self._buffer) == 0:
                return []

            batch_size = min(batch_size, len(self._buffer))

            if strategy == "priority":
                samples = self._priority_sample(batch_size)
            elif strategy == "recent":
                samples = list(self._buffer)[-batch_size:]
            else:  # uniform
                indices = random.sample(range(len(self._buffer)), batch_size)
                samples = [self._buffer[i] for i in indices]

            self._total_sampled += len(samples)
            return samples

    def _priority_sample(self, batch_size: int) -> List[Experience]:
        """Sample based on priorities."""
        buffer_list = list(self._buffer)

        # Get priorities with temporal decay
        now = datetime.now()
        weights = []
        for exp in buffer_list:
            base_priority = self._priorities.get(exp.id, 1.0)
            age_days = (now - exp.timestamp).days
            decayed_priority = base_priority * (self.temporal_decay ** age_days)
            weights.append(decayed_priority)

        # Normalize
        total = sum(weights)
        if total == 0:
            probs = [1.0 / len(weights)] * len(weights)
        else:
            probs = [w / total for w in weights]

        # Sample
        indices = random.choices(range(len(buffer_list)), weights=probs, k=batch_size)
        return [buffer_list[i] for i in indices]

    def _get_base_priority(self, exp: Experience) -> float:
        """Get base priority from experience type."""
        priority_map = {
            ExperiencePriority.CRITICAL: 10.0,
            ExperiencePriority.HIGH: 5.0,
            ExperiencePriority.MEDIUM: 2.0,
            ExperiencePriority.LOW: 1.0,
            ExperiencePriority.BACKGROUND: 0.5,
        }
        return priority_map.get(exp.priority, 1.0)

    def _compute_quality_score(self, exp: Experience) -> float:
        """Compute quality score for experience."""
        score = 1.0

        # Boost for corrections
        if exp.correction:
            score *= 2.0

        # Boost for explicit feedback
        if exp.feedback_type == "explicit":
            score *= 1.5
        elif exp.feedback_type == "correction":
            score *= 2.0

        # Penalize very short responses
        if len(exp.response) < 10:
            score *= 0.5

        # Boost high reward
        if exp.reward > 0.8:
            score *= 1.3

        return min(score, 5.0)  # Cap at 5x

    def _cleanup_priorities(self) -> None:
        """Remove priorities for experiences no longer in buffer."""
        buffer_ids = {exp.id for exp in self._buffer}
        self._priorities = {
            k: v for k, v in self._priorities.items() if k in buffer_ids
        }

    async def get_preference_pairs(self, max_pairs: int = 1000) -> List[PreferencePair]:
        """Extract preference pairs from corrections."""
        async with self._lock:
            pairs = []
            for exp in self._buffer:
                pair = exp.to_preference_pair()
                if pair:
                    pairs.append(pair)
                if len(pairs) >= max_pairs:
                    break
            return pairs

    async def get_by_difficulty(
        self,
        min_difficulty: float,
        max_difficulty: float,
        limit: int = 100,
    ) -> List[Experience]:
        """Get experiences within difficulty range for curriculum learning."""
        async with self._lock:
            matching = [
                exp for exp in self._buffer
                if min_difficulty <= exp.difficulty_score <= max_difficulty
            ]
            return matching[:limit]

    def __len__(self) -> int:
        return len(self._buffer)

    def get_stats(self) -> Dict[str, Any]:
        """Get buffer statistics."""
        priority_counts = defaultdict(int)
        safety_counts = defaultdict(int)

        for exp in self._buffer:
            priority_counts[exp.priority.value] += 1
            safety_counts[exp.safety_tier.value] += 1

        return {
            "size": len(self._buffer),
            "max_size": self.max_size,
            "total_added": self._total_added,
            "total_sampled": self._total_sampled,
            "duplicates_rejected": self._duplicates_rejected,
            "quality_rejected": self._quality_rejected,
            "priority_distribution": dict(priority_counts),
            "safety_distribution": dict(safety_counts),
            "fill_rate": len(self._buffer) / self.max_size,
        }


# =============================================================================
# DPO TRAINER - Direct Preference Optimization
# =============================================================================

@dataclass
class DPOConfig:
    """Configuration for DPO training."""
    beta: float = 0.1  # KL penalty coefficient
    label_smoothing: float = 0.0  # Label smoothing for preferences
    loss_type: str = "sigmoid"  # sigmoid, hinge, ipo, kto
    reference_free: bool = False  # Skip reference model
    sync_ref_model: bool = False  # Sync reference model during training
    precompute_ref_log_probs: bool = True  # Precompute for efficiency

    # Advanced DPO variants
    use_ipo: bool = False  # Identity Preference Optimization
    ipo_tau: float = 0.5  # IPO temperature

    use_kto: bool = False  # Kahneman-Tversky Optimization
    kto_desirable_weight: float = 1.0
    kto_undesirable_weight: float = 1.0

    use_orpo: bool = False  # Odds Ratio Preference Optimization
    orpo_alpha: float = 0.1

    # Training params
    learning_rate: float = 5e-7
    batch_size: int = 4
    gradient_accumulation_steps: int = 4
    max_length: int = 2048
    max_prompt_length: int = 1024

    # Regularization
    max_grad_norm: float = 1.0
    weight_decay: float = 0.01


class DPOTrainer:
    """
    Direct Preference Optimization Trainer.

    Implements DPO and variants (IPO, KTO, ORPO) for preference learning
    without explicit reward modeling.

    Key Features:
    - Reference model management (frozen copy)
    - Multiple loss variants
    - Efficient batching for preference pairs
    - Gradient accumulation
    - Mixed precision training
    """

    def __init__(
        self,
        model: nn.Module,
        tokenizer: Any,
        config: DPOConfig,
        reference_model: Optional[nn.Module] = None,
        device: Optional[torch.device] = None,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.device = device or self._detect_device()

        # Reference model (frozen copy for KL computation)
        if reference_model is not None:
            self.reference_model = reference_model
        elif not config.reference_free:
            self.reference_model = self._create_reference_model()
        else:
            self.reference_model = None

        # Move to device
        self.model.to(self.device)
        if self.reference_model:
            self.reference_model.to(self.device)
            self.reference_model.eval()
            for param in self.reference_model.parameters():
                param.requires_grad = False

        # Optimizer
        self.optimizer = self._create_optimizer()

        # Metrics tracking
        self._step = 0
        self._metrics_history: List[TrainingMetrics] = []

        logger.info(f"DPO Trainer initialized with {config.loss_type} loss")

    def _detect_device(self) -> torch.device:
        """Detect optimal device."""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return torch.device("mps")
        return torch.device("cpu")

    def _create_reference_model(self) -> nn.Module:
        """Create frozen reference model."""
        import copy
        ref_model = copy.deepcopy(self.model)
        ref_model.eval()
        return ref_model

    def _create_optimizer(self) -> torch.optim.Optimizer:
        """Create optimizer with weight decay."""
        # Separate parameters for weight decay
        decay_params = []
        no_decay_params = []

        for name, param in self.model.named_parameters():
            if not param.requires_grad:
                continue
            if "bias" in name or "LayerNorm" in name or "layer_norm" in name:
                no_decay_params.append(param)
            else:
                decay_params.append(param)

        optimizer_groups = [
            {"params": decay_params, "weight_decay": self.config.weight_decay},
            {"params": no_decay_params, "weight_decay": 0.0},
        ]

        return torch.optim.AdamW(
            optimizer_groups,
            lr=self.config.learning_rate,
            betas=(0.9, 0.999),
            eps=1e-8,
        )

    def _get_batch_log_probs(
        self,
        model: nn.Module,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        labels: torch.Tensor,
    ) -> torch.Tensor:
        """Compute log probabilities for a batch."""
        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_dict=True,
            )
            logits = outputs.logits

        # Shift for causal LM
        shift_logits = logits[..., :-1, :].contiguous()
        shift_labels = labels[..., 1:].contiguous()
        shift_mask = attention_mask[..., 1:].contiguous()

        # Compute log probs
        log_probs = F.log_softmax(shift_logits, dim=-1)

        # Gather log probs for actual tokens
        per_token_log_probs = torch.gather(
            log_probs, dim=-1, index=shift_labels.unsqueeze(-1)
        ).squeeze(-1)

        # Mask padding and average
        per_token_log_probs = per_token_log_probs * shift_mask
        sequence_log_probs = per_token_log_probs.sum(dim=-1) / shift_mask.sum(dim=-1).clamp(min=1)

        return sequence_log_probs

    def _dpo_loss_sigmoid(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        reference_chosen_logps: torch.Tensor,
        reference_rejected_logps: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Standard DPO loss with sigmoid."""
        # Log ratios
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps

        # DPO loss
        logits = pi_logratios - ref_logratios

        if self.config.label_smoothing > 0:
            # Label smoothing
            losses = (
                -F.logsigmoid(self.config.beta * logits) * (1 - self.config.label_smoothing)
                - F.logsigmoid(-self.config.beta * logits) * self.config.label_smoothing
            )
        else:
            losses = -F.logsigmoid(self.config.beta * logits)

        # Rewards for metrics
        chosen_rewards = self.config.beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = self.config.beta * (policy_rejected_logps - reference_rejected_logps).detach()

        return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()

    def _dpo_loss_hinge(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        reference_chosen_logps: torch.Tensor,
        reference_rejected_logps: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Hinge loss variant of DPO."""
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps

        logits = pi_logratios - ref_logratios
        losses = torch.relu(1 - self.config.beta * logits)

        chosen_rewards = self.config.beta * (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = self.config.beta * (policy_rejected_logps - reference_rejected_logps).detach()

        return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()

    def _ipo_loss(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        reference_chosen_logps: torch.Tensor,
        reference_rejected_logps: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Identity Preference Optimization loss."""
        pi_logratios = policy_chosen_logps - policy_rejected_logps
        ref_logratios = reference_chosen_logps - reference_rejected_logps

        # IPO loss: (logits - tau)^2
        logits = pi_logratios - ref_logratios
        losses = (logits - self.config.ipo_tau) ** 2

        chosen_rewards = (policy_chosen_logps - reference_chosen_logps).detach()
        rejected_rewards = (policy_rejected_logps - reference_rejected_logps).detach()

        return losses.mean(), chosen_rewards.mean(), rejected_rewards.mean()

    def _kto_loss(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        reference_chosen_logps: torch.Tensor,
        reference_rejected_logps: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Kahneman-Tversky Optimization loss."""
        # KL divergences
        kl_chosen = policy_chosen_logps - reference_chosen_logps
        kl_rejected = policy_rejected_logps - reference_rejected_logps

        # KTO loss components
        chosen_loss = 1 - F.sigmoid(self.config.beta * kl_chosen)
        rejected_loss = 1 - F.sigmoid(-self.config.beta * kl_rejected)

        loss = (
            self.config.kto_desirable_weight * chosen_loss.mean() +
            self.config.kto_undesirable_weight * rejected_loss.mean()
        )

        return loss, kl_chosen.mean().detach(), kl_rejected.mean().detach()

    def _orpo_loss(
        self,
        policy_chosen_logps: torch.Tensor,
        policy_rejected_logps: torch.Tensor,
        chosen_nll_loss: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Odds Ratio Preference Optimization loss."""
        # Log odds ratio
        log_odds = (policy_chosen_logps - policy_rejected_logps) - (
            torch.log1p(-torch.exp(policy_chosen_logps)) -
            torch.log1p(-torch.exp(policy_rejected_logps))
        )

        # ORPO loss = NLL + alpha * odds_ratio_loss
        odds_ratio_loss = -F.logsigmoid(log_odds).mean()
        loss = chosen_nll_loss + self.config.orpo_alpha * odds_ratio_loss

        return loss, policy_chosen_logps.mean().detach(), policy_rejected_logps.mean().detach()

    def compute_loss(
        self,
        batch: Dict[str, torch.Tensor],
    ) -> Tuple[torch.Tensor, TrainingMetrics]:
        """Compute DPO loss for a batch."""
        # Get log probs for chosen and rejected
        policy_chosen_logps = self._get_batch_log_probs(
            self.model,
            batch["chosen_input_ids"],
            batch["chosen_attention_mask"],
            batch["chosen_labels"],
        )

        policy_rejected_logps = self._get_batch_log_probs(
            self.model,
            batch["rejected_input_ids"],
            batch["rejected_attention_mask"],
            batch["rejected_labels"],
        )

        # Reference model log probs
        if self.reference_model is not None and not self.config.reference_free:
            with torch.no_grad():
                reference_chosen_logps = self._get_batch_log_probs(
                    self.reference_model,
                    batch["chosen_input_ids"],
                    batch["chosen_attention_mask"],
                    batch["chosen_labels"],
                )
                reference_rejected_logps = self._get_batch_log_probs(
                    self.reference_model,
                    batch["rejected_input_ids"],
                    batch["rejected_attention_mask"],
                    batch["rejected_labels"],
                )
        else:
            # Reference-free: use zeros
            reference_chosen_logps = torch.zeros_like(policy_chosen_logps)
            reference_rejected_logps = torch.zeros_like(policy_rejected_logps)

        # Compute loss based on type
        if self.config.use_ipo:
            loss, chosen_rewards, rejected_rewards = self._ipo_loss(
                policy_chosen_logps, policy_rejected_logps,
                reference_chosen_logps, reference_rejected_logps,
            )
        elif self.config.use_kto:
            loss, chosen_rewards, rejected_rewards = self._kto_loss(
                policy_chosen_logps, policy_rejected_logps,
                reference_chosen_logps, reference_rejected_logps,
            )
        elif self.config.loss_type == "hinge":
            loss, chosen_rewards, rejected_rewards = self._dpo_loss_hinge(
                policy_chosen_logps, policy_rejected_logps,
                reference_chosen_logps, reference_rejected_logps,
            )
        else:  # sigmoid (default)
            loss, chosen_rewards, rejected_rewards = self._dpo_loss_sigmoid(
                policy_chosen_logps, policy_rejected_logps,
                reference_chosen_logps, reference_rejected_logps,
            )

        # Compute accuracy
        with torch.no_grad():
            reward_diff = policy_chosen_logps - policy_rejected_logps
            accuracy = (reward_diff > 0).float().mean()

        metrics = TrainingMetrics(
            step=self._step,
            loss=loss.item(),
            chosen_rewards=chosen_rewards.item(),
            rejected_rewards=rejected_rewards.item(),
            accuracy=accuracy.item(),
            kl_divergence=(policy_chosen_logps - reference_chosen_logps).mean().item(),
        )

        return loss, metrics

    def train_step(self, batch: Dict[str, torch.Tensor]) -> TrainingMetrics:
        """Execute single training step."""
        self.model.train()

        # Move batch to device
        batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}

        # Forward pass
        loss, metrics = self.compute_loss(batch)

        # Backward pass
        loss = loss / self.config.gradient_accumulation_steps
        loss.backward()

        # Update weights every N steps
        self._step += 1
        if self._step % self.config.gradient_accumulation_steps == 0:
            # Gradient clipping
            if self.config.max_grad_norm > 0:
                grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), self.config.max_grad_norm
                )
                metrics.gradient_norm = grad_norm.item()

            self.optimizer.step()
            self.optimizer.zero_grad()

        self._metrics_history.append(metrics)
        return metrics

    async def train_on_preferences(
        self,
        preference_pairs: List[PreferencePair],
        num_epochs: int = 1,
        progress_callback: Optional[Callable[[TrainingMetrics], Awaitable[None]]] = None,
    ) -> List[TrainingMetrics]:
        """Train on a list of preference pairs."""
        from torch.utils.data import DataLoader

        logger.info(f"Starting DPO training on {len(preference_pairs)} pairs for {num_epochs} epochs")

        # Create dataset
        dataset = self._create_preference_dataset(preference_pairs)
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            collate_fn=self._collate_preferences,
        )

        all_metrics = []

        for epoch in range(num_epochs):
            epoch_losses = []

            for batch in dataloader:
                metrics = self.train_step(batch)
                metrics.epoch = epoch
                epoch_losses.append(metrics.loss)
                all_metrics.append(metrics)

                if progress_callback:
                    await progress_callback(metrics)

            avg_loss = sum(epoch_losses) / len(epoch_losses)
            logger.info(f"Epoch {epoch + 1}/{num_epochs} - Avg Loss: {avg_loss:.4f}")

        return all_metrics

    def _create_preference_dataset(
        self,
        pairs: List[PreferencePair],
    ) -> Dataset:
        """Create dataset from preference pairs."""

        class PreferenceDataset(Dataset):
            def __init__(self, pairs: List[PreferencePair], tokenizer, max_length: int):
                self.pairs = pairs
                self.tokenizer = tokenizer
                self.max_length = max_length

            def __len__(self):
                return len(self.pairs)

            def __getitem__(self, idx: int) -> Dict[str, Any]:
                pair = self.pairs[idx]

                # Tokenize chosen
                chosen_text = f"{pair.prompt}\n{pair.chosen}"
                chosen = self.tokenizer(
                    chosen_text,
                    max_length=self.max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt",
                )

                # Tokenize rejected
                rejected_text = f"{pair.prompt}\n{pair.rejected}"
                rejected = self.tokenizer(
                    rejected_text,
                    max_length=self.max_length,
                    padding="max_length",
                    truncation=True,
                    return_tensors="pt",
                )

                return {
                    "chosen_input_ids": chosen["input_ids"].squeeze(0),
                    "chosen_attention_mask": chosen["attention_mask"].squeeze(0),
                    "chosen_labels": chosen["input_ids"].squeeze(0),
                    "rejected_input_ids": rejected["input_ids"].squeeze(0),
                    "rejected_attention_mask": rejected["attention_mask"].squeeze(0),
                    "rejected_labels": rejected["input_ids"].squeeze(0),
                }

        return PreferenceDataset(pairs, self.tokenizer, self.config.max_length)

    def _collate_preferences(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        """Collate preference batch."""
        return {
            key: torch.stack([item[key] for item in batch])
            for key in batch[0].keys()
        }

    def save(self, path: Path) -> None:
        """Save trainer state."""
        path = Path(path)
        path.mkdir(parents=True, exist_ok=True)

        # Save model
        self.model.save_pretrained(path / "model")
        self.tokenizer.save_pretrained(path / "model")

        # Save optimizer
        torch.save(self.optimizer.state_dict(), path / "optimizer.pt")

        # Save config and metrics
        with open(path / "config.json", "w") as f:
            json.dump(self.config.__dict__, f, indent=2)

        logger.info(f"DPO Trainer saved to {path}")

    def load(self, path: Path) -> None:
        """Load trainer state."""
        path = Path(path)

        # Load optimizer
        optimizer_path = path / "optimizer.pt"
        if optimizer_path.exists():
            self.optimizer.load_state_dict(torch.load(optimizer_path))

        logger.info(f"DPO Trainer loaded from {path}")


# =============================================================================
# RLHF PIPELINE - Full Reinforcement Learning from Human Feedback
# =============================================================================

@dataclass
class RLHFConfig:
    """Configuration for RLHF training."""
    # Reward model
    reward_model_path: Optional[str] = None
    train_reward_model: bool = True
    reward_batch_size: int = 8
    reward_learning_rate: float = 1e-5
    reward_epochs: int = 1

    # PPO
    ppo_epochs: int = 4
    ppo_batch_size: int = 4
    ppo_learning_rate: float = 1e-6
    ppo_clip_range: float = 0.2
    ppo_value_clip_range: float = 0.2
    ppo_entropy_coef: float = 0.01
    ppo_value_coef: float = 0.5
    ppo_max_grad_norm: float = 0.5

    # KL penalty
    init_kl_coef: float = 0.2
    target_kl: float = 6.0
    adaptive_kl: bool = True

    # GAE
    gamma: float = 1.0
    gae_lambda: float = 0.95

    # Generation
    max_new_tokens: int = 256
    temperature: float = 0.7
    top_p: float = 0.9

    # Training
    total_steps: int = 10000
    eval_every: int = 500
    save_every: int = 1000


class RewardModel(nn.Module):
    """
    Reward model for RLHF.

    Architecture: Base LLM + Scalar value head
    """

    def __init__(
        self,
        base_model: nn.Module,
        hidden_size: int = 4096,
        dropout: float = 0.1,
    ):
        super().__init__()
        self.base_model = base_model

        # Value head
        self.value_head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, 1),
        )

        # Freeze base model initially
        for param in self.base_model.parameters():
            param.requires_grad = False

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
    ) -> torch.Tensor:
        """Forward pass returning scalar rewards."""
        # Get hidden states
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )

        # Use last hidden state at last token position
        hidden_states = outputs.hidden_states[-1]

        # Find last non-padding token
        sequence_lengths = attention_mask.sum(dim=1) - 1
        batch_indices = torch.arange(hidden_states.size(0), device=hidden_states.device)
        last_hidden = hidden_states[batch_indices, sequence_lengths]

        # Compute reward
        rewards = self.value_head(last_hidden).squeeze(-1)

        return rewards

    def compute_loss(
        self,
        chosen_ids: torch.Tensor,
        chosen_mask: torch.Tensor,
        rejected_ids: torch.Tensor,
        rejected_mask: torch.Tensor,
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """Compute reward model loss."""
        chosen_rewards = self.forward(chosen_ids, chosen_mask)
        rejected_rewards = self.forward(rejected_ids, rejected_mask)

        # Bradley-Terry loss
        loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()

        # Metrics
        accuracy = ((chosen_rewards > rejected_rewards).float().mean()).item()

        return loss, {
            "loss": loss.item(),
            "accuracy": accuracy,
            "chosen_reward_mean": chosen_rewards.mean().item(),
            "rejected_reward_mean": rejected_rewards.mean().item(),
        }


class PPOTrainer:
    """
    Proximal Policy Optimization trainer for RLHF.

    Implements the full PPO algorithm with:
    - Clipped surrogate objective
    - Value function clipping
    - GAE advantage estimation
    - KL penalty with adaptive coefficient
    """

    def __init__(
        self,
        policy_model: nn.Module,
        value_model: nn.Module,
        reward_model: RewardModel,
        tokenizer: Any,
        config: RLHFConfig,
        device: Optional[torch.device] = None,
    ):
        self.policy = policy_model
        self.value = value_model
        self.reward_model = reward_model
        self.tokenizer = tokenizer
        self.config = config
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Reference model for KL
        import copy
        self.ref_policy = copy.deepcopy(policy_model)
        self.ref_policy.eval()
        for param in self.ref_policy.parameters():
            param.requires_grad = False

        # Move to device
        self.policy.to(self.device)
        self.value.to(self.device)
        self.reward_model.to(self.device)
        self.ref_policy.to(self.device)

        # Optimizers
        self.policy_optimizer = torch.optim.AdamW(
            self.policy.parameters(),
            lr=config.ppo_learning_rate,
        )
        self.value_optimizer = torch.optim.AdamW(
            self.value.parameters(),
            lr=config.ppo_learning_rate * 2,
        )

        # KL coefficient
        self.kl_coef = config.init_kl_coef

        # Metrics
        self._step = 0

    def _compute_advantages(
        self,
        rewards: torch.Tensor,
        values: torch.Tensor,
        dones: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """Compute GAE advantages and returns."""
        batch_size = rewards.size(0)
        advantages = torch.zeros_like(rewards)
        last_gae = 0

        for t in reversed(range(batch_size)):
            if t == batch_size - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.config.gamma * next_value * (1 - dones[t]) - values[t]
            advantages[t] = last_gae = delta + self.config.gamma * self.config.gae_lambda * (1 - dones[t]) * last_gae

        returns = advantages + values

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        return advantages, returns

    def _compute_policy_loss(
        self,
        log_probs: torch.Tensor,
        old_log_probs: torch.Tensor,
        advantages: torch.Tensor,
    ) -> Tuple[torch.Tensor, Dict[str, float]]:
        """Compute clipped PPO policy loss."""
        # Ratio
        ratio = torch.exp(log_probs - old_log_probs)

        # Clipped surrogate
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.config.ppo_clip_range, 1 + self.config.ppo_clip_range) * advantages

        policy_loss = -torch.min(surr1, surr2).mean()

        # Entropy bonus
        entropy = -(log_probs * torch.exp(log_probs)).mean()

        # Total loss
        loss = policy_loss - self.config.ppo_entropy_coef * entropy

        return loss, {
            "policy_loss": policy_loss.item(),
            "entropy": entropy.item(),
            "ratio_mean": ratio.mean().item(),
            "clip_fraction": ((ratio - 1).abs() > self.config.ppo_clip_range).float().mean().item(),
        }

    def _compute_value_loss(
        self,
        values: torch.Tensor,
        old_values: torch.Tensor,
        returns: torch.Tensor,
    ) -> torch.Tensor:
        """Compute clipped value loss."""
        # Clipped value loss
        value_clipped = old_values + torch.clamp(
            values - old_values,
            -self.config.ppo_value_clip_range,
            self.config.ppo_value_clip_range,
        )

        value_loss1 = (values - returns) ** 2
        value_loss2 = (value_clipped - returns) ** 2

        return torch.max(value_loss1, value_loss2).mean()

    async def generate_rollout(
        self,
        prompts: List[str],
    ) -> Dict[str, torch.Tensor]:
        """Generate rollout data from prompts."""
        self.policy.eval()

        # Tokenize prompts
        encoded = self.tokenizer(
            prompts,
            padding=True,
            truncation=True,
            return_tensors="pt",
        ).to(self.device)

        # Generate responses
        with torch.no_grad():
            outputs = self.policy.generate(
                **encoded,
                max_new_tokens=self.config.max_new_tokens,
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                return_dict_in_generate=True,
                output_scores=True,
            )

        generated_ids = outputs.sequences

        # Compute rewards
        rewards = self.reward_model(
            generated_ids,
            (generated_ids != self.tokenizer.pad_token_id).long(),
        )

        # Compute log probs
        # ... (truncated for brevity, full implementation would compute log probs)

        return {
            "input_ids": generated_ids,
            "rewards": rewards,
        }

    def train_step(
        self,
        rollout: Dict[str, torch.Tensor],
    ) -> TrainingMetrics:
        """Execute single PPO training step."""
        self.policy.train()
        self.value.train()

        # ... Full PPO step implementation
        # This is a simplified version

        self._step += 1

        return TrainingMetrics(
            step=self._step,
            # ... metrics
        )

    def update_kl_coef(self, kl_div: float) -> None:
        """Adaptively update KL coefficient."""
        if not self.config.adaptive_kl:
            return

        if kl_div > self.config.target_kl * 1.5:
            self.kl_coef *= 1.5
        elif kl_div < self.config.target_kl / 1.5:
            self.kl_coef /= 1.5

        self.kl_coef = max(0.01, min(10.0, self.kl_coef))


class RLHFPipeline:
    """
    Complete RLHF pipeline orchestrating reward model training and PPO.

    Pipeline:
    1. Collect human preferences
    2. Train reward model on preferences
    3. Use PPO to optimize policy against reward model
    4. Iterate with new preferences
    """

    def __init__(
        self,
        policy_model: nn.Module,
        tokenizer: Any,
        config: RLHFConfig,
        device: Optional[torch.device] = None,
    ):
        self.policy = policy_model
        self.tokenizer = tokenizer
        self.config = config
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Initialize components
        self.reward_model: Optional[RewardModel] = None
        self.ppo_trainer: Optional[PPOTrainer] = None

        # Experience buffer
        self.experience_buffer = ExperienceBuffer()

        logger.info("RLHF Pipeline initialized")

    async def train_reward_model(
        self,
        preference_pairs: List[PreferencePair],
        progress_callback: Optional[Callable] = None,
    ) -> Dict[str, float]:
        """Train the reward model on preferences."""
        logger.info(f"Training reward model on {len(preference_pairs)} pairs")

        # Initialize reward model if needed
        if self.reward_model is None:
            hidden_size = self.policy.config.hidden_size
            self.reward_model = RewardModel(
                base_model=self.policy,
                hidden_size=hidden_size,
            ).to(self.device)

        # Create optimizer
        optimizer = torch.optim.AdamW(
            self.reward_model.value_head.parameters(),
            lr=self.config.reward_learning_rate,
        )

        # Training loop
        metrics = {"loss": [], "accuracy": []}

        for epoch in range(self.config.reward_epochs):
            for i in range(0, len(preference_pairs), self.config.reward_batch_size):
                batch = preference_pairs[i:i + self.config.reward_batch_size]

                # Prepare batch
                chosen_texts = [f"{p.prompt}\n{p.chosen}" for p in batch]
                rejected_texts = [f"{p.prompt}\n{p.rejected}" for p in batch]

                chosen_enc = self.tokenizer(
                    chosen_texts, padding=True, truncation=True,
                    max_length=self.config.max_new_tokens * 2, return_tensors="pt"
                ).to(self.device)

                rejected_enc = self.tokenizer(
                    rejected_texts, padding=True, truncation=True,
                    max_length=self.config.max_new_tokens * 2, return_tensors="pt"
                ).to(self.device)

                # Compute loss
                loss, batch_metrics = self.reward_model.compute_loss(
                    chosen_enc["input_ids"], chosen_enc["attention_mask"],
                    rejected_enc["input_ids"], rejected_enc["attention_mask"],
                )

                # Backward
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                metrics["loss"].append(batch_metrics["loss"])
                metrics["accuracy"].append(batch_metrics["accuracy"])

                if progress_callback:
                    await progress_callback(batch_metrics)

        final_metrics = {
            "final_loss": sum(metrics["loss"]) / len(metrics["loss"]),
            "final_accuracy": sum(metrics["accuracy"]) / len(metrics["accuracy"]),
        }

        logger.info(f"Reward model trained - Loss: {final_metrics['final_loss']:.4f}, Accuracy: {final_metrics['final_accuracy']:.2%}")

        return final_metrics

    async def run_ppo_training(
        self,
        prompts: List[str],
        num_steps: int,
        progress_callback: Optional[Callable] = None,
    ) -> List[TrainingMetrics]:
        """Run PPO training loop."""
        if self.reward_model is None:
            raise ValueError("Reward model must be trained first")

        # Initialize PPO trainer
        import copy
        value_model = copy.deepcopy(self.policy)

        self.ppo_trainer = PPOTrainer(
            policy_model=self.policy,
            value_model=value_model,
            reward_model=self.reward_model,
            tokenizer=self.tokenizer,
            config=self.config,
            device=self.device,
        )

        all_metrics = []

        for step in range(num_steps):
            # Sample prompts
            batch_prompts = random.sample(prompts, min(self.config.ppo_batch_size, len(prompts)))

            # Generate rollout
            rollout = await self.ppo_trainer.generate_rollout(batch_prompts)

            # Train step
            metrics = self.ppo_trainer.train_step(rollout)
            all_metrics.append(metrics)

            if progress_callback:
                await progress_callback(metrics)

            if step % 100 == 0:
                logger.info(f"PPO Step {step}/{num_steps}")

        return all_metrics


# =============================================================================
# CONSTITUTIONAL AI - Self-Supervised Safety Alignment
# =============================================================================

@dataclass
class ConstitutionalPrinciple:
    """A principle for Constitutional AI."""
    id: str
    name: str
    description: str
    critique_prompt: str
    revision_prompt: str
    priority: int = 5  # 1-10, lower is higher priority
    category: str = "general"  # safety, helpfulness, honesty, harmlessness


DEFAULT_CONSTITUTIONAL_PRINCIPLES = [
    ConstitutionalPrinciple(
        id="harmlessness_1",
        name="No Harmful Instructions",
        description="Responses should not provide instructions for harmful activities",
        critique_prompt="Does this response provide instructions that could cause harm to people or property?",
        revision_prompt="Revise the response to remove any potentially harmful instructions while remaining helpful.",
        priority=1,
        category="harmlessness",
    ),
    ConstitutionalPrinciple(
        id="honesty_1",
        name="Truthfulness",
        description="Responses should be truthful and not spread misinformation",
        critique_prompt="Does this response contain any false or misleading information?",
        revision_prompt="Revise the response to be more accurate and truthful.",
        priority=2,
        category="honesty",
    ),
    ConstitutionalPrinciple(
        id="safety_1",
        name="Safety Awareness",
        description="Responses should include appropriate safety warnings",
        critique_prompt="Does this response adequately address safety concerns where relevant?",
        revision_prompt="Add appropriate safety information or warnings to the response.",
        priority=2,
        category="safety",
    ),
    ConstitutionalPrinciple(
        id="helpfulness_1",
        name="Helpful and Informative",
        description="Responses should be genuinely helpful to the user",
        critique_prompt="Is this response as helpful as it could be to the user?",
        revision_prompt="Revise the response to be more helpful and informative.",
        priority=3,
        category="helpfulness",
    ),
    ConstitutionalPrinciple(
        id="privacy_1",
        name="Privacy Protection",
        description="Responses should not reveal or request sensitive personal information",
        critique_prompt="Does this response inappropriately handle personal or private information?",
        revision_prompt="Revise the response to better protect user privacy.",
        priority=1,
        category="safety",
    ),
]


class ConstitutionalAITrainer:
    """
    Constitutional AI training pipeline.

    Implements the CAI approach:
    1. Generate initial response
    2. Self-critique against constitutional principles
    3. Revise response based on critique
    4. Create preference pair (revised > original)
    5. Train with DPO on preference pairs

    This is self-supervised - no human labeling required after principles defined.
    """

    def __init__(
        self,
        model: nn.Module,
        tokenizer: Any,
        principles: Optional[List[ConstitutionalPrinciple]] = None,
        device: Optional[torch.device] = None,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.principles = principles or DEFAULT_CONSTITUTIONAL_PRINCIPLES
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Sort principles by priority
        self.principles.sort(key=lambda p: p.priority)

        # DPO trainer for preference learning
        self.dpo_trainer: Optional[DPOTrainer] = None

        # Statistics
        self._critique_count = 0
        self._revision_count = 0
        self._pairs_generated = 0

        logger.info(f"Constitutional AI Trainer initialized with {len(self.principles)} principles")

    async def generate_response(self, prompt: str) -> str:
        """Generate initial response."""
        self.model.eval()

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response[len(prompt):].strip()

    async def critique_response(
        self,
        prompt: str,
        response: str,
        principle: ConstitutionalPrinciple,
    ) -> Tuple[bool, str]:
        """
        Critique response against a principle.

        Returns (needs_revision, critique_text)
        """
        critique_prompt = f"""Given this conversation:

User: {prompt}
Assistant: {response}

{principle.critique_prompt}

Provide a brief critique. Start with "Yes" if there's an issue, or "No" if it's fine.
"""

        critique = await self.generate_response(critique_prompt)
        self._critique_count += 1

        needs_revision = critique.lower().startswith("yes")
        return needs_revision, critique

    async def revise_response(
        self,
        prompt: str,
        response: str,
        principle: ConstitutionalPrinciple,
        critique: str,
    ) -> str:
        """Revise response based on critique."""
        revision_prompt = f"""Given this conversation:

User: {prompt}
Original Assistant Response: {response}

Critique: {critique}

{principle.revision_prompt}

Provide the revised response:
"""

        revised = await self.generate_response(revision_prompt)
        self._revision_count += 1

        return revised

    async def constitutional_revision(
        self,
        prompt: str,
        initial_response: str,
    ) -> Tuple[str, List[str]]:
        """
        Apply constitutional revision process.

        Returns (final_response, list of applied principles)
        """
        current_response = initial_response
        applied_principles = []

        for principle in self.principles:
            needs_revision, critique = await self.critique_response(
                prompt, current_response, principle
            )

            if needs_revision:
                current_response = await self.revise_response(
                    prompt, current_response, principle, critique
                )
                applied_principles.append(principle.name)

        return current_response, applied_principles

    async def generate_preference_pairs(
        self,
        prompts: List[str],
        progress_callback: Optional[Callable] = None,
    ) -> List[PreferencePair]:
        """Generate preference pairs through constitutional revision."""
        pairs = []

        for i, prompt in enumerate(prompts):
            # Generate initial response
            initial_response = await self.generate_response(prompt)

            # Apply constitutional revision
            revised_response, applied = await self.constitutional_revision(
                prompt, initial_response
            )

            # Only create pair if revision was made
            if applied:
                pair = PreferencePair(
                    prompt=prompt,
                    chosen=revised_response,
                    rejected=initial_response,
                    metadata={"applied_principles": applied},
                    source="constitutional_ai",
                )
                pairs.append(pair)
                self._pairs_generated += 1

            if progress_callback:
                await progress_callback({
                    "processed": i + 1,
                    "total": len(prompts),
                    "pairs_generated": len(pairs),
                })

        logger.info(f"Generated {len(pairs)} preference pairs from {len(prompts)} prompts")
        return pairs

    async def train(
        self,
        prompts: List[str],
        num_iterations: int = 3,
        pairs_per_iteration: int = 100,
        dpo_epochs: int = 1,
        progress_callback: Optional[Callable] = None,
    ) -> List[TrainingMetrics]:
        """
        Run full Constitutional AI training loop.

        Each iteration:
        1. Generate preference pairs through self-critique
        2. Train on pairs with DPO
        """
        # Initialize DPO trainer
        if self.dpo_trainer is None:
            self.dpo_trainer = DPOTrainer(
                model=self.model,
                tokenizer=self.tokenizer,
                config=DPOConfig(),
                device=self.device,
            )

        all_metrics = []

        for iteration in range(num_iterations):
            logger.info(f"Constitutional AI iteration {iteration + 1}/{num_iterations}")

            # Sample prompts for this iteration
            sample_prompts = random.sample(prompts, min(pairs_per_iteration, len(prompts)))

            # Generate preference pairs
            pairs = await self.generate_preference_pairs(sample_prompts)

            if len(pairs) == 0:
                logger.warning("No preference pairs generated, skipping DPO training")
                continue

            # Train with DPO
            metrics = await self.dpo_trainer.train_on_preferences(
                pairs,
                num_epochs=dpo_epochs,
                progress_callback=progress_callback,
            )
            all_metrics.extend(metrics)

        return all_metrics

    def get_stats(self) -> Dict[str, Any]:
        """Get training statistics."""
        return {
            "critique_count": self._critique_count,
            "revision_count": self._revision_count,
            "pairs_generated": self._pairs_generated,
            "principles_count": len(self.principles),
        }


# =============================================================================
# CURRICULUM LEARNING - Progressive Difficulty Training
# =============================================================================

@dataclass
class CurriculumConfig:
    """Configuration for curriculum learning."""
    strategy: CurriculumStrategy = CurriculumStrategy.ADAPTIVE
    initial_difficulty: float = 0.1  # Start with easy examples
    final_difficulty: float = 1.0  # End with hardest
    difficulty_step: float = 0.1  # How much to increase per epoch
    competence_threshold: float = 0.8  # Required accuracy to advance
    patience: int = 3  # Epochs without improvement before advancing
    anti_curriculum_prob: float = 0.1  # Probability of sampling hard examples early


class DifficultyScorer:
    """
    Score difficulty of training examples.

    Uses multiple signals:
    - Response length (longer = harder)
    - Vocabulary complexity
    - Task type complexity
    - Historical model performance
    """

    def __init__(self):
        self._performance_history: Dict[str, List[float]] = defaultdict(list)

    def score(self, experience: Experience) -> float:
        """Compute difficulty score for an experience."""
        scores = []

        # Length-based difficulty
        response_length = len(experience.response.split())
        length_score = min(1.0, response_length / 500)
        scores.append(length_score)

        # Vocabulary complexity (unique words ratio)
        words = experience.response.lower().split()
        if len(words) > 0:
            unique_ratio = len(set(words)) / len(words)
            scores.append(unique_ratio)

        # Task type (from metadata)
        task_difficulty = {
            "simple_qa": 0.2,
            "explanation": 0.4,
            "reasoning": 0.6,
            "coding": 0.7,
            "math": 0.8,
            "creative": 0.5,
        }
        task_type = experience.metadata.get("task_type", "general")
        scores.append(task_difficulty.get(task_type, 0.5))

        # Historical performance (if available)
        exp_hash = hashlib.sha256(experience.prompt.encode()).hexdigest()[:8]
        if exp_hash in self._performance_history:
            # Lower accuracy = higher difficulty
            avg_acc = sum(self._performance_history[exp_hash]) / len(self._performance_history[exp_hash])
            scores.append(1 - avg_acc)

        return sum(scores) / len(scores)

    def update_performance(self, experience: Experience, accuracy: float) -> None:
        """Update performance history for adaptive difficulty."""
        exp_hash = hashlib.sha256(experience.prompt.encode()).hexdigest()[:8]
        self._performance_history[exp_hash].append(accuracy)

        # Keep only recent history
        if len(self._performance_history[exp_hash]) > 10:
            self._performance_history[exp_hash] = self._performance_history[exp_hash][-10:]


class CurriculumScheduler:
    """
    Schedule training data by difficulty level.

    Implements multiple curriculum strategies for optimal learning progression.
    """

    def __init__(
        self,
        config: CurriculumConfig,
        experience_buffer: ExperienceBuffer,
        difficulty_scorer: Optional[DifficultyScorer] = None,
    ):
        self.config = config
        self.buffer = experience_buffer
        self.scorer = difficulty_scorer or DifficultyScorer()

        # State
        self._current_difficulty = config.initial_difficulty
        self._current_epoch = 0
        self._competence_scores: List[float] = []
        self._no_improvement_count = 0

        logger.info(f"Curriculum Scheduler initialized with {config.strategy.value} strategy")

    def get_current_difficulty_range(self) -> Tuple[float, float]:
        """Get current difficulty range for sampling."""
        if self.config.strategy == CurriculumStrategy.LINEAR:
            progress = self._current_epoch * self.config.difficulty_step
            center = min(self.config.final_difficulty, self.config.initial_difficulty + progress)
        elif self.config.strategy == CurriculumStrategy.EXPONENTIAL:
            progress = 1 - math.exp(-self._current_epoch * 0.1)
            center = self.config.initial_difficulty + progress * (self.config.final_difficulty - self.config.initial_difficulty)
        elif self.config.strategy == CurriculumStrategy.ADAPTIVE:
            center = self._current_difficulty
        elif self.config.strategy == CurriculumStrategy.ANTI_CURRICULUM:
            # Start with hard, move to easy
            progress = self._current_epoch * self.config.difficulty_step
            center = max(self.config.initial_difficulty, self.config.final_difficulty - progress)
        else:
            center = self._current_difficulty

        # Window around center
        window = 0.2
        return (max(0, center - window), min(1, center + window))

    async def sample_batch(self, batch_size: int) -> List[Experience]:
        """Sample batch according to curriculum."""
        min_diff, max_diff = self.get_current_difficulty_range()

        # Get experiences in difficulty range
        experiences = await self.buffer.get_by_difficulty(min_diff, max_diff, limit=batch_size * 3)

        if len(experiences) < batch_size:
            # Fall back to priority sampling if not enough in range
            additional = await self.buffer.sample(batch_size - len(experiences), strategy="priority")
            experiences.extend(additional)

        # Anti-curriculum: occasionally sample hard examples
        if self.config.strategy != CurriculumStrategy.ANTI_CURRICULUM:
            if random.random() < self.config.anti_curriculum_prob:
                hard_samples = await self.buffer.get_by_difficulty(0.8, 1.0, limit=batch_size // 4)
                experiences = experiences[:batch_size - len(hard_samples)] + hard_samples

        return experiences[:batch_size]

    def update_competence(self, accuracy: float) -> None:
        """Update competence estimate and potentially advance difficulty."""
        self._competence_scores.append(accuracy)

        if len(self._competence_scores) < 3:
            return

        # Rolling average
        recent_competence = sum(self._competence_scores[-5:]) / min(5, len(self._competence_scores))

        if self.config.strategy == CurriculumStrategy.ADAPTIVE:
            if recent_competence >= self.config.competence_threshold:
                # Advance difficulty
                self._current_difficulty = min(
                    self.config.final_difficulty,
                    self._current_difficulty + self.config.difficulty_step
                )
                self._no_improvement_count = 0
                logger.info(f"Curriculum advanced to difficulty {self._current_difficulty:.2f}")
            else:
                self._no_improvement_count += 1

                if self._no_improvement_count >= self.config.patience:
                    # Still advance but slower
                    self._current_difficulty = min(
                        self.config.final_difficulty,
                        self._current_difficulty + self.config.difficulty_step / 2
                    )
                    self._no_improvement_count = 0
                    logger.info(f"Curriculum advanced (patience) to {self._current_difficulty:.2f}")

        elif self.config.strategy == CurriculumStrategy.SELF_PACED:
            # Model controls pace based on loss
            if recent_competence > 0.9:
                self._current_difficulty = min(1.0, self._current_difficulty + 0.05)
            elif recent_competence < 0.5:
                self._current_difficulty = max(0.1, self._current_difficulty - 0.05)

    def advance_epoch(self) -> None:
        """Advance to next epoch."""
        self._current_epoch += 1

    def get_stats(self) -> Dict[str, Any]:
        """Get scheduler statistics."""
        return {
            "strategy": self.config.strategy.value,
            "current_difficulty": self._current_difficulty,
            "current_epoch": self._current_epoch,
            "recent_competence": (
                sum(self._competence_scores[-5:]) / min(5, len(self._competence_scores))
                if self._competence_scores else 0
            ),
            "difficulty_range": self.get_current_difficulty_range(),
        }


# =============================================================================
# FSDP - Fully Sharded Data Parallel
# =============================================================================

@dataclass
class FSDPConfig:
    """Configuration for Fully Sharded Data Parallel training."""
    enabled: bool = True
    sharding_strategy: str = "FULL_SHARD"  # FULL_SHARD, SHARD_GRAD_OP, NO_SHARD
    cpu_offload: bool = False
    backward_prefetch: str = "BACKWARD_PRE"  # BACKWARD_PRE, BACKWARD_POST
    mixed_precision: str = "fp16"  # fp16, bf16, fp32
    auto_wrap_policy: str = "transformer"  # transformer, size_based
    min_num_params: int = 1000000  # For size-based wrapping
    sync_module_states: bool = True
    use_orig_params: bool = True
    limit_all_gathers: bool = True
    activation_checkpointing: bool = True


class FSDPWrapper:
    """
    Wrapper for Fully Sharded Data Parallel training.

    Enables training of large models across multiple GPUs/nodes
    with memory-efficient parameter sharding.
    """

    def __init__(
        self,
        model: nn.Module,
        config: FSDPConfig,
    ):
        self.config = config
        self._original_model = model
        self._fsdp_model: Optional[nn.Module] = None
        self._is_initialized = False

    def initialize(self) -> nn.Module:
        """Initialize FSDP wrapping."""
        if not self.config.enabled:
            return self._original_model

        if not torch.cuda.is_available():
            logger.warning("FSDP requires CUDA, falling back to regular model")
            return self._original_model

        try:
            from torch.distributed.fsdp import (
                FullyShardedDataParallel as FSDP,
                ShardingStrategy,
                BackwardPrefetch,
                MixedPrecision,
                CPUOffload,
            )
            from torch.distributed.fsdp.wrap import (
                transformer_auto_wrap_policy,
                size_based_auto_wrap_policy,
            )
        except ImportError:
            logger.warning("FSDP not available, using regular model")
            return self._original_model

        # Sharding strategy
        strategy_map = {
            "FULL_SHARD": ShardingStrategy.FULL_SHARD,
            "SHARD_GRAD_OP": ShardingStrategy.SHARD_GRAD_OP,
            "NO_SHARD": ShardingStrategy.NO_SHARD,
        }
        sharding_strategy = strategy_map.get(
            self.config.sharding_strategy, ShardingStrategy.FULL_SHARD
        )

        # Backward prefetch
        prefetch_map = {
            "BACKWARD_PRE": BackwardPrefetch.BACKWARD_PRE,
            "BACKWARD_POST": BackwardPrefetch.BACKWARD_POST,
        }
        backward_prefetch = prefetch_map.get(
            self.config.backward_prefetch, BackwardPrefetch.BACKWARD_PRE
        )

        # Mixed precision
        if self.config.mixed_precision == "fp16":
            mp_policy = MixedPrecision(
                param_dtype=torch.float16,
                reduce_dtype=torch.float16,
                buffer_dtype=torch.float16,
            )
        elif self.config.mixed_precision == "bf16":
            mp_policy = MixedPrecision(
                param_dtype=torch.bfloat16,
                reduce_dtype=torch.bfloat16,
                buffer_dtype=torch.bfloat16,
            )
        else:
            mp_policy = None

        # CPU offload
        cpu_offload = CPUOffload(offload_params=True) if self.config.cpu_offload else None

        # Auto wrap policy
        if self.config.auto_wrap_policy == "transformer":
            # Get transformer layer class
            layer_cls = self._get_transformer_layer_cls()
            if layer_cls:
                auto_wrap = transformer_auto_wrap_policy(
                    transformer_layer_cls={layer_cls}
                )
            else:
                auto_wrap = size_based_auto_wrap_policy(
                    min_num_params=self.config.min_num_params
                )
        else:
            auto_wrap = size_based_auto_wrap_policy(
                min_num_params=self.config.min_num_params
            )

        # Wrap model
        self._fsdp_model = FSDP(
            self._original_model,
            sharding_strategy=sharding_strategy,
            cpu_offload=cpu_offload,
            auto_wrap_policy=auto_wrap,
            backward_prefetch=backward_prefetch,
            mixed_precision=mp_policy,
            sync_module_states=self.config.sync_module_states,
            use_orig_params=self.config.use_orig_params,
            limit_all_gathers=self.config.limit_all_gathers,
        )

        # Activation checkpointing
        if self.config.activation_checkpointing:
            self._apply_activation_checkpointing()

        self._is_initialized = True
        logger.info(f"FSDP initialized with {self.config.sharding_strategy} strategy")

        return self._fsdp_model

    def _get_transformer_layer_cls(self) -> Optional[type]:
        """Get the transformer layer class for auto-wrapping."""
        # Try common layer names
        for name, module in self._original_model.named_modules():
            cls_name = module.__class__.__name__
            if "DecoderLayer" in cls_name or "TransformerBlock" in cls_name:
                return module.__class__
        return None

    def _apply_activation_checkpointing(self) -> None:
        """Apply activation checkpointing to FSDP model."""
        if self._fsdp_model is None:
            return

        try:
            from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
                checkpoint_wrapper,
                apply_activation_checkpointing,
                CheckpointImpl,
            )

            layer_cls = self._get_transformer_layer_cls()
            if layer_cls:
                apply_activation_checkpointing(
                    self._fsdp_model,
                    checkpoint_wrapper_fn=checkpoint_wrapper,
                    check_fn=lambda module: isinstance(module, layer_cls),
                )
                logger.info("Activation checkpointing applied")
        except ImportError:
            logger.warning("Activation checkpointing not available")

    @property
    def model(self) -> nn.Module:
        """Get the wrapped model."""
        return self._fsdp_model if self._is_initialized else self._original_model

    def save_checkpoint(self, path: Path, optimizer: Optional[torch.optim.Optimizer] = None) -> None:
        """Save FSDP checkpoint."""
        if not self._is_initialized:
            torch.save(self._original_model.state_dict(), path / "model.pt")
            return

        try:
            from torch.distributed.fsdp import (
                FullStateDictConfig,
                StateDictType,
            )
            from torch.distributed.fsdp.api import FullOptimStateDictConfig

            # Full state dict for saving
            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)

            with FSDP.state_dict_type(
                self._fsdp_model,
                StateDictType.FULL_STATE_DICT,
                save_policy,
            ):
                state_dict = self._fsdp_model.state_dict()

                if torch.distributed.get_rank() == 0:
                    path.mkdir(parents=True, exist_ok=True)
                    torch.save(state_dict, path / "model.pt")

                    if optimizer:
                        optim_state = FSDP.full_optim_state_dict(
                            self._fsdp_model, optimizer
                        )
                        torch.save(optim_state, path / "optimizer.pt")

            logger.info(f"FSDP checkpoint saved to {path}")

        except Exception as e:
            logger.error(f"Failed to save FSDP checkpoint: {e}")
            raise


# =============================================================================
# UNIFIED ADVANCED TRAINER - Orchestrates All Methods
# =============================================================================

@dataclass
class AdvancedTrainingConfig:
    """Unified configuration for advanced training."""
    # Method selection
    training_method: TrainingMethod = TrainingMethod.DPO

    # Sub-configs
    dpo_config: DPOConfig = field(default_factory=DPOConfig)
    rlhf_config: RLHFConfig = field(default_factory=RLHFConfig)
    fsdp_config: FSDPConfig = field(default_factory=FSDPConfig)
    curriculum_config: CurriculumConfig = field(default_factory=CurriculumConfig)

    # General training
    num_epochs: int = 3
    batch_size: int = 4
    learning_rate: float = 2e-5
    max_grad_norm: float = 1.0

    # Experience buffer
    experience_buffer_size: int = 100000
    min_experiences_to_train: int = 100

    # Checkpointing
    checkpoint_dir: Path = field(default_factory=lambda: Path.home() / ".jarvis" / "training" / "advanced")
    save_every_n_steps: int = 500

    # Evaluation
    eval_every_n_steps: int = 100
    eval_batch_size: int = 8


class AdvancedTrainer:
    """
    Unified Advanced Trainer orchestrating all training methods.

    Supports:
    - Automatic method selection based on available data
    - Seamless switching between methods
    - Continuous learning integration
    - Curriculum-aware sampling
    - FSDP for large-scale training
    """

    def __init__(
        self,
        model: nn.Module,
        tokenizer: Any,
        config: AdvancedTrainingConfig,
        device: Optional[torch.device] = None,
    ):
        self.model = model
        self.tokenizer = tokenizer
        self.config = config
        self.device = device or self._detect_device()

        # Initialize components
        self.experience_buffer = ExperienceBuffer(max_size=config.experience_buffer_size)
        self.curriculum_scheduler = CurriculumScheduler(
            config=config.curriculum_config,
            experience_buffer=self.experience_buffer,
        )

        # Initialize FSDP if enabled
        if config.fsdp_config.enabled and torch.cuda.device_count() > 1:
            fsdp_wrapper = FSDPWrapper(model, config.fsdp_config)
            self.model = fsdp_wrapper.initialize()

        # Method-specific trainers (lazy initialized)
        self._dpo_trainer: Optional[DPOTrainer] = None
        self._rlhf_pipeline: Optional[RLHFPipeline] = None
        self._constitutional_trainer: Optional[ConstitutionalAITrainer] = None

        # Training state
        self._step = 0
        self._epoch = 0
        self._metrics_history: List[TrainingMetrics] = []

        logger.info(f"Advanced Trainer initialized with {config.training_method.value} method")

    def _detect_device(self) -> torch.device:
        """Detect optimal device."""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
            return torch.device("mps")
        return torch.device("cpu")

    def _get_trainer(self) -> Union[DPOTrainer, RLHFPipeline, ConstitutionalAITrainer]:
        """Get or create the appropriate trainer."""
        method = self.config.training_method

        if method == TrainingMethod.DPO:
            if self._dpo_trainer is None:
                self._dpo_trainer = DPOTrainer(
                    model=self.model,
                    tokenizer=self.tokenizer,
                    config=self.config.dpo_config,
                    device=self.device,
                )
            return self._dpo_trainer

        elif method == TrainingMethod.RLHF:
            if self._rlhf_pipeline is None:
                self._rlhf_pipeline = RLHFPipeline(
                    policy_model=self.model,
                    tokenizer=self.tokenizer,
                    config=self.config.rlhf_config,
                    device=self.device,
                )
            return self._rlhf_pipeline

        elif method == TrainingMethod.CONSTITUTIONAL:
            if self._constitutional_trainer is None:
                self._constitutional_trainer = ConstitutionalAITrainer(
                    model=self.model,
                    tokenizer=self.tokenizer,
                    device=self.device,
                )
            return self._constitutional_trainer

        else:
            raise ValueError(f"Unsupported training method: {method}")

    async def add_experience(self, experience: Experience) -> bool:
        """Add experience to buffer."""
        # Score difficulty for curriculum
        experience.difficulty_score = self.curriculum_scheduler.scorer.score(experience)

        return await self.experience_buffer.add(experience)

    async def add_experiences_from_jarvis(
        self,
        interactions: List[Dict[str, Any]],
    ) -> int:
        """Add experiences from JARVIS interaction logs."""
        added = 0

        for interaction in interactions:
            exp = Experience(
                prompt=interaction.get("user_input", ""),
                response=interaction.get("assistant_output", ""),
                reward=interaction.get("reward", 0.5),
                correction=interaction.get("correction"),
                feedback_type=interaction.get("feedback_type", "implicit"),
                metadata=interaction.get("metadata", {}),
            )

            # Determine priority
            if exp.correction:
                exp.priority = ExperiencePriority.CRITICAL
            elif exp.feedback_type == "explicit":
                exp.priority = ExperiencePriority.HIGH
            elif exp.reward < 0.3:
                exp.priority = ExperiencePriority.HIGH

            if await self.add_experience(exp):
                added += 1

        logger.info(f"Added {added}/{len(interactions)} experiences from JARVIS")
        return added

    async def train_step(
        self,
        progress_callback: Optional[Callable[[TrainingMetrics], Awaitable[None]]] = None,
    ) -> TrainingMetrics:
        """Execute single training step based on configured method."""
        method = self.config.training_method

        if method == TrainingMethod.DPO:
            # Get preference pairs from buffer
            pairs = await self.experience_buffer.get_preference_pairs(
                max_pairs=self.config.batch_size * 10
            )

            if len(pairs) < self.config.batch_size:
                logger.warning("Not enough preference pairs for DPO training")
                return TrainingMetrics(step=self._step)

            trainer = self._get_trainer()
            metrics_list = await trainer.train_on_preferences(
                pairs,
                num_epochs=1,
                progress_callback=progress_callback,
            )
            metrics = metrics_list[-1] if metrics_list else TrainingMetrics()

        elif method == TrainingMethod.RLHF:
            # Sample prompts from buffer
            experiences = await self.curriculum_scheduler.sample_batch(self.config.batch_size)
            prompts = [exp.prompt for exp in experiences]

            pipeline = self._get_trainer()

            # Train reward model first if needed
            pairs = await self.experience_buffer.get_preference_pairs()
            if pairs:
                await pipeline.train_reward_model(pairs)

            # Run PPO
            metrics_list = await pipeline.run_ppo_training(
                prompts, num_steps=10, progress_callback=progress_callback
            )
            metrics = metrics_list[-1] if metrics_list else TrainingMetrics()

        elif method == TrainingMethod.CONSTITUTIONAL:
            # Get prompts from buffer
            experiences = await self.experience_buffer.sample(self.config.batch_size * 2)
            prompts = [exp.prompt for exp in experiences]

            trainer = self._get_trainer()
            metrics_list = await trainer.train(
                prompts,
                num_iterations=1,
                pairs_per_iteration=self.config.batch_size,
                progress_callback=progress_callback,
            )
            metrics = metrics_list[-1] if metrics_list else TrainingMetrics()

        else:
            raise ValueError(f"Unsupported method: {method}")

        # Update state
        self._step += 1
        metrics.step = self._step
        metrics.experience_buffer_size = len(self.experience_buffer)
        metrics.curriculum_level = self.curriculum_scheduler._current_difficulty

        # Update curriculum
        self.curriculum_scheduler.update_competence(metrics.accuracy)

        # Track metrics
        self._metrics_history.append(metrics)

        # Checkpoint
        if self._step % self.config.save_every_n_steps == 0:
            await self.save_checkpoint()

        return metrics

    async def train(
        self,
        num_steps: Optional[int] = None,
        progress_callback: Optional[Callable[[TrainingMetrics], Awaitable[None]]] = None,
    ) -> List[TrainingMetrics]:
        """Run full training loop."""
        num_steps = num_steps or self.config.num_epochs * 1000

        logger.info(f"Starting advanced training for {num_steps} steps")

        all_metrics = []

        for step in range(num_steps):
            # Check if enough data
            if len(self.experience_buffer) < self.config.min_experiences_to_train:
                logger.warning(f"Waiting for more experiences ({len(self.experience_buffer)}/{self.config.min_experiences_to_train})")
                await asyncio.sleep(10)
                continue

            metrics = await self.train_step(progress_callback)
            all_metrics.append(metrics)

            if step % 100 == 0:
                logger.info(
                    f"Step {step}/{num_steps} - Loss: {metrics.loss:.4f}, "
                    f"Accuracy: {metrics.accuracy:.2%}, Curriculum: {metrics.curriculum_level:.2f}"
                )

        return all_metrics

    async def save_checkpoint(self) -> Path:
        """Save training checkpoint."""
        checkpoint_path = self.config.checkpoint_dir / f"checkpoint-{self._step}"
        checkpoint_path.mkdir(parents=True, exist_ok=True)

        # Save model
        if hasattr(self.model, "save_pretrained"):
            self.model.save_pretrained(checkpoint_path / "model")
        else:
            torch.save(self.model.state_dict(), checkpoint_path / "model.pt")

        # Save tokenizer
        self.tokenizer.save_pretrained(checkpoint_path / "model")

        # Save trainer state
        state = {
            "step": self._step,
            "epoch": self._epoch,
            "config": {
                "training_method": self.config.training_method.value,
                "batch_size": self.config.batch_size,
            },
            "curriculum": self.curriculum_scheduler.get_stats(),
            "buffer_stats": self.experience_buffer.get_stats(),
        }

        with open(checkpoint_path / "trainer_state.json", "w") as f:
            json.dump(state, f, indent=2)

        logger.info(f"Checkpoint saved to {checkpoint_path}")
        return checkpoint_path

    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get summary of training metrics."""
        if not self._metrics_history:
            return {}

        recent = self._metrics_history[-100:]

        return {
            "total_steps": self._step,
            "current_epoch": self._epoch,
            "training_method": self.config.training_method.value,
            "recent_loss": sum(m.loss for m in recent) / len(recent),
            "recent_accuracy": sum(m.accuracy for m in recent) / len(recent),
            "curriculum_level": self.curriculum_scheduler._current_difficulty,
            "buffer_size": len(self.experience_buffer),
            "buffer_stats": self.experience_buffer.get_stats(),
        }


# =============================================================================
# EXPORTS
# =============================================================================

__all__ = [
    # Enums
    "TrainingMethod",
    "SafetyTier",
    "CurriculumStrategy",
    "ExperiencePriority",
    # Data structures
    "PreferencePair",
    "Experience",
    "TrainingBatch",
    "TrainingMetrics",
    # Experience Buffer
    "ExperienceBuffer",
    # DPO
    "DPOConfig",
    "DPOTrainer",
    # RLHF
    "RLHFConfig",
    "RewardModel",
    "PPOTrainer",
    "RLHFPipeline",
    # Constitutional AI
    "ConstitutionalPrinciple",
    "ConstitutionalAITrainer",
    "DEFAULT_CONSTITUTIONAL_PRINCIPLES",
    # Curriculum Learning
    "CurriculumConfig",
    "DifficultyScorer",
    "CurriculumScheduler",
    # FSDP
    "FSDPConfig",
    "FSDPWrapper",
    # Unified Trainer
    "AdvancedTrainingConfig",
    "AdvancedTrainer",
]
